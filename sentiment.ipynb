{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "88051c3d-4a44-4ec6-8adb-c1d38653e94d",
    "_uuid": "71fff663eb9075c63db6ddd0904379ffbcdcad20"
   },
   "source": [
    "# Machine Learning Challenge\n",
    "\n",
    "## Overview\n",
    "\n",
    "The focus of this exercise is on a field within machine learning called [Natural Language Processing](https://en.wikipedia.org/wiki/Natural-language_processing). We can think of this field as the intersection between language, and machine learning. Tasks in this field include automatic translation (Google translate), intelligent personal assistants (Siri), information extraction, and speech recognition for example.\n",
    "\n",
    "NLP uses many of the same techniques as traditional data science, but also features a number of specialised skills and approaches. There is no expectation that you have any experience with NLP, however, to complete the challenge it will be useful to have the following skills:\n",
    "\n",
    "- understanding of the python programming language\n",
    "- understanding of basic machine learning concepts, i.e. supervised learning\n",
    "\n",
    "\n",
    "### Instructions\n",
    "\n",
    "1. Download this notebook!\n",
    "2. Answer each of the provided questions, including your source code as cells in this notebook.\n",
    "3. Share the results with us, e.g. a Github repo.\n",
    "\n",
    "### Task description\n",
    "\n",
    "You will be performing a task known as [sentiment analysis](https://en.wikipedia.org/wiki/Sentiment_analysis). Here, the goal is to predict sentiment -- the emotional intent behind a statement -- from text. For example, the sentence: \"*This movie was terrible!\"* has a negative sentiment, whereas \"*loved this cinematic masterpiece*\" has a positive sentiment.\n",
    "\n",
    "To simplify the task, we consider sentiment binary: labels of `1` indicate a sentence has a positive sentiment, and labels of `0` indicate that the sentence has a negative sentiment.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "The dataset is split across three files, representing three different sources -- Amazon, Yelp and IMDB. Your task is to build a sentiment analysis model using both the Yelp and IMDB data as your training-set, and test the performance of your model on the Amazon data.\n",
    "\n",
    "Each file can be found in the `input` directory, and contains 1000 rows of data. Each row contains a sentence, a `tab` character and then a label -- `0` or `1`. \n",
    "\n",
    "**Notes**\n",
    "- Feel free to use existing machine learning libraries as components in you solution!\n",
    "- Suggested libraries: `sklearn` (for machine learning), `pandas` (for loading/processing data), `spacy` (for text processing).\n",
    "- As mentioned, you are not expected to have previous experience with this exact task. You are free to refer to external tutorials/resources to assist you. However, you will be asked to justfify the choices you have made -- so make you understand the approach you have taken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amazon_cells_labelled.txt', 'yelp_labelled.txt', 'imdb_labelled.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.listdir(\"./input\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "So there is no way for me to plug it in here in the US unless I go by a converter.\t0\r\n",
      "Good case, Excellent value.\t1\r\n",
      "Great for the jawbone.\t1\r\n",
      "Tied to charger for conversations lasting more than 45 minutes.MAJOR PROBLEMS!!\t0\r\n",
      "The mic is great.\t1\r\n",
      "I have to jiggle the plug to get it to line up right to get decent volume.\t0\r\n",
      "If you have several dozen or several hundred contacts, then imagine the fun of sending each of them one by one.\t0\r\n",
      "If you are Razr owner...you must have this!\t1\r\n",
      "Needless to say, I wasted my money.\t0\r\n",
      "What a waste of money and time!.\t0\r\n"
     ]
    }
   ],
   "source": [
    "!head \"./input/amazon_cells_labelled.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "387106cd-e89a-462f-b204-a91a73d12137",
    "_uuid": "cbd1a4b1d16ce7db6def7b3b393b48618d7e4777"
   },
   "source": [
    "# Tasks\n",
    "### 1. Read and concatenate data into test and train sets.\n",
    "### 2. Prepare the data for input into your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**install libraries**\n",
    "\n",
    "```shell\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install nltk\n",
    "!pip install sklearn\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens.token import Token\n",
    "from nltk import ngrams\n",
    "from dataclasses import dataclass\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Sentence:\n",
    "    sentence: str\n",
    "    label: int\n",
    "    tokens: List[Token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_line(line: str):\n",
    "    sentence, label = line.strip().split('\\t')\n",
    "    label = int(label)\n",
    "    tokens = nlp(sentence)\n",
    "    return Sentence(sentence, label, tokens)\n",
    "\n",
    "\n",
    "def load_sentiment_data(*files: str):\n",
    "    data = (\n",
    "        parse_line(line)\n",
    "        for f in files \n",
    "        for line in open(f, 'r')\n",
    "    )\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load train set and test set, for only 1000 records in each file, just load to memory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = list(load_sentiment_data('input/imdb_labelled.txt', 'input/yelp_labelled.txt'))\n",
    "test_set = list(load_sentiment_data('input/amazon_cells_labelled.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*To get all tag of spacy*\n",
    "\n",
    "for label in nlp.get_pipe(\"tagger\").labels:\n",
    "    print(label, \" -- \", spacy.explain(label))\n",
    "    \n",
    "*To get all dep of spacy*\n",
    "\n",
    "for label in nlp.get_pipe(\"parser\").labels:\n",
    "    print(label, \" -- \", spacy.explain(label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a8240a39-7002-435b-ba45-ac859d209f7f",
    "_uuid": "69c6d7ea240a191abfaaf00574f09521944387d7"
   },
   "source": [
    "#### 2a: Find the ten most frequent words in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Wordcount by tag and dep, and then show top 20 for each of the dep/tag*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_wordcount = {}\n",
    "dep_wordcount = {}\n",
    "\n",
    "for row in train_set:\n",
    "    for token in row.tokens:\n",
    "        dep = token.dep_\n",
    "        tag = token.tag_\n",
    "        lemma = token.lemma_\n",
    "        tag_wordcount.setdefault(tag, {})[lemma] = tag_wordcount.get(tag, {}).get(lemma, 0) + 1\n",
    "        dep_wordcount.setdefault(dep, {})[lemma] = tag_wordcount.get(dep, {}).get(lemma, 0) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print wordcount for each of the tag and dep**\n",
    "\n",
    "```python\n",
    "print('Top frequent words for each tag_:')\n",
    "for tag, counts in tag_wordcount.items():\n",
    "    print(tag, '>>', sorted(counts.items(), key=lambda x: -x[1])[:40])\n",
    "\n",
    "print('Top frequent words for each dep_:')\n",
    "for dep, counts in dep_wordcount.items():\n",
    "    print(dep, '>>', sorted(counts.items(), key=lambda x: -x[1])[:40])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reiew the wordcount for each of the tegs, and got those tags and white list\n",
    "\n",
    "# better to review the partial dep as well when there is spare time\n",
    "\n",
    "all_tags = {\n",
    "    'RB', 'JJ', 'RBR', 'JJS', 'WRB', 'RBS', 'JJR', 'UH'\n",
    "}\n",
    "partial_tags = {\n",
    "    'VBD': {'love', 'find', 'enjoy', 'think', 'like', 'feel', 'see', 'say', 'suck', 'wait', 'look', 'want', 'seem', 'lack', 'keep', 'need', 'leave', 'try', 'taste'},\n",
    "    'NN': {'everything', 'nothing', 'anyone', 'quality', 'lot', 'bit', 'part'},\n",
    "    'VBG': {'wait', 'consider', 'feel', 'check', 'make', 'end', 'think', 'waste', 'lose', 'follow', 'singe', 'remain'},\n",
    "    'DT': {'all', 'no', 'some', 'any', 'every', 'another', 'those', 'these', 'both', 'each', 'either', 'somethat', 'neither'},\n",
    "    'VBN': {'recommend', 'waste', 'disappoint', 'leave', 'involve', 'lose', 'rate', 'tell', 'go', 'become', 'suppose', 'throw', 'treat'},\n",
    "    'CC': {'but', 'or', 'both', 'plus', 'yet', 'so', 'either', '+', 'nor'},\n",
    "    'IN': {'without', 'throughout', 'although', 'whether', 'under', 'until', 'towards', 'unless', 'despite'},\n",
    "    'VB': {'think', 'recommend', 'avoid', 'love', 'like', 'waste', 'feel', 'find', 'wait', 'enjoy', 'want', 'leave', 'return', 'care', 'mention'},\n",
    "    'VBZ': {'make', 'suck', 'lack', 'give', 'fail', 'deserve', 'seem', 'rank', 'leave', 'like', 'stink', 'keep', 'cease', 'love', 'smell'},\n",
    "    'WDT': {'whatever', 'what'},\n",
    "    'VBP': {'think', 'love', 'want', 'like', 'recommend', 'guess', 'make', 'look', 'give', 'believe', 'hate', 'find', 'hope', 'need', 'wish', 'mean', 'enjoy', 'understand', 'feel', 'consider', 'rate', 'expect', 'struggle', 'doubt'}\n",
    "}\n",
    "all_deps = {'advmod', 'amod', 'acomp', 'intj', 'preconj', 'predet', 'oprd'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**stop words (the, a, this, it, etc) are eliminated as we have a whitelist**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train your model and justify your choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3a: Generate features, use single word and n-gram in the above `all_tags`/`partial_tags` list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_feature(tokens: List[Token]):\n",
    "    feature = []\n",
    "    for tok in tokens:\n",
    "        if tok.tag_ in all_tags or tok.tag_ in partial_tags and tok.lemma in partial_tags[tok.tag_] or tok.dep_ in all_deps:\n",
    "            feature.append(tok.text.lower())\n",
    "    if feature:\n",
    "        return ';;'.join(sorted(feature))\n",
    "    return None\n",
    "\n",
    "\n",
    "def generate_features(tokens: List[Token]):\n",
    "    tokens = list(filter(lambda x: x.dep_ != 'punct', tokens))\n",
    "    features = []\n",
    "    for i in range(1, n_gram):\n",
    "        for words in ngrams(tokens, i):\n",
    "            feature = as_feature(words)\n",
    "            if feature is not None:\n",
    "                features.append(feature)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vector = {}\n",
    "for row in train_set:\n",
    "    for feature in generate_features(row.tokens):\n",
    "        feature_vector[feature] = feature_vector.get(feature, 0) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a view of the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vectore_desc = sorted(feature_vector.items(), key=lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20: [(\"n't\", 1415), ('not', 1043), ('good', 749), ('great', 671), ('just', 566), ('so', 447), ('bad', 426), ('ever', 400), ('very', 399), ('here', 389), ('really', 378), ('only', 330), ('when', 325), ('back', 316), ('best', 315), ('even', 293), ('all', 290), ('other', 289), ('never', 287), ('more', 275)] \n",
      "Mid: [('special;;whatsoever', 3), ('probably;;worst', 3), ('fresh;;succulent', 3), ('equally;;special', 3), ('enthusiastic;;real', 3), ('all;;small', 3), ('elegantly;;tiny', 3), ('eggplant;;usual', 3), ('at;;mediocre', 3), ('not;;now', 3), ('as;;friendly', 3), ('even;;hi', 3), ('incredible;;nay', 3), ('nay;;transcendant', 3), ('incredible;;nay;;transcendant', 3), ('full;;petty', 3), ('happy;;hungry', 3), ('sore;;still', 3), ('friendly;;professional', 3), ('furthermore', 3), ('delicious;;so', 3), ('just;;spicy', 3), ('gooodd;;so', 3), ('insulted;;so', 3), ('enough;;fresh', 3), ('creamy;;smooth', 3), ('quite;;really', 3), ('nice;;quite;;really', 3), ('first;;only', 3), ('just;;rather', 3), ('just;;much;;rather', 3), (\"n't;;small\", 3), ('good;;rarely', 3), ('also;;really', 3), ('also;;good;;really', 3), ('-;;multi', 3), ('-;;grain', 3), ('-;;grain;;multi', 3), ('friendly;;so', 3), ('impressed;;very', 3)] \n",
      "Last 20: [('not;;really;;sweet', 1), ('not;;really;;spicy', 1), ('not;;really;;sweet;;too', 1), ('not;;really;;spicy;;sweet', 1), ('enough;;not;;really;;spicy', 1), ('horrible;;overpriced', 1), ('just;;maybe', 1), ('at;;busy', 1), (\"all;;at;;busy;;n't\", 1), ('all;;at;;now', 1), ('also;;dirty;;outside', 1), ('always;;friendly;;helpful', 1), ('douchey;;more', 1), ('back;;then', 1), ('not;;rude', 1), ('not;;rude;;very', 1), ('even;;not;;rude', 1), (\"fresh;;n't;;obviously\", 1), ('not;;overall', 1), ('underwhelming;;whole', 1)]\n"
     ]
    }
   ],
   "source": [
    "print('Top 20:', feature_vectore_desc[:20], '\\nMid:', feature_vectore_desc[2200:2240], '\\nLast 20:', feature_vectore_desc[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature dimension with n-gram (n is [1, 4]): 3694\n"
     ]
    }
   ],
   "source": [
    "print(f'Feature dimension with n-gram (n is [1, {n_gram-1}]):', len(feature_vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Generate n-dimension traning vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = len(feature_vector)\n",
    "feature2index = dict(zip(feature_vector.keys(), range(num_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillin_features(num_features, tokens: List[Token], feature2index):\n",
    "    row_array = [0 for _ in range(num_features)]\n",
    "    for feature in generate_features(tokens):\n",
    "        if feature in feature2index:\n",
    "            row_array[feature2index[feature]] = 1\n",
    "    return row_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = [\n",
    "    fillin_features(num_features, row.tokens, feature2index)\n",
    "    for row in train_set\n",
    "]\n",
    "y_train = [row.label for row in train_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_model = LogisticRegression(solver='lbfgs').fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.915"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_model.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "1a5840b2-c84c-42f6-9fc9-4fed64e48298",
    "_uuid": "f4eeecd64b54cc05098affe6cca4c40204af8ecf"
   },
   "source": [
    "### 4. Evaluate your model using metric(s) you see fit and justify your choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = [\n",
    "    fillin_features(num_features, row.tokens, feature2index)\n",
    "    for row in test_set\n",
    "]\n",
    "y_test = [row.label for row in test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.747"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
